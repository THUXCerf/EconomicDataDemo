{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd0213a9",
   "metadata": {},
   "source": [
    "# Text Mining Demo\n",
    "\n",
    "This dataset is downloaded from: [ctrip hotel review](https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv), which has over 7000 hotel reviews, over 5000 positive reviews and over 2000 negative reviews. The data looks roughly like the following: \n",
    "\n",
    "The first column is the label, which takes 0 or 1, 0 means negative reviews, 1 means positive reviews. The second column is the content of the comments. In this small demo, 20 positive and 20 negative comments were randomly chosen for text mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d86d0c",
   "metadata": {},
   "source": [
    "![](./tf-idf.png)\n",
    "***Fig 1. Typical Process of Text Mining***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df990d29",
   "metadata": {},
   "source": [
    "**Term Frequency (TF) $\\text{tf}_{i, j}$**: the number of occurrences of term $t_i$ in document $d_j$\n",
    "\n",
    "**Inverse Document Frequency (IDF) for term $t_i$** is:\n",
    "$$\n",
    "\\text{idf}_i=log_2\\frac{|D|}{|\\{d|t_i\\in d\\}|}\n",
    "$$\n",
    "where $|D|$ is the total number of documents, $|\\{d|t_i\\in d\\}|$ is the number of documents where term $t_i$ appears.\n",
    "\n",
    "**Term Frequency - Inverse Document Frequency (TF-IDF)** is defined as:\n",
    "$$\n",
    "\\text{tf-idf}=\\text{tf}_{i,j}\\cdot \\text{idf}_i\n",
    "$$\n",
    "\n",
    "Created by *Xinghao YU*, July 18th, 2023\n",
    "\n",
    "***Copyright @ The Chinese University of Hong Kong, Shenzhen***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a76ab11",
   "metadata": {},
   "source": [
    "## Load Dependencies and Configuration Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91f8409",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:09.136797Z",
     "start_time": "2023-07-17T15:51:08.336942Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(10086)\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c750ed17",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ab93a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:09.170158Z",
     "start_time": "2023-07-17T15:51:09.137615Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_all = pd.read_csv('./ChnSentiCorp_htl_all.csv')\n",
    "\n",
    "print('评论数目（总体）：%d' % pd_all.shape[0])\n",
    "print('评论数目（正向）：%d' % pd_all[pd_all.label == 1].shape[0])\n",
    "print('评论数目（负向）：%d' % pd_all[pd_all.label == 0].shape[0])\n",
    "\n",
    "pd_all.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e612a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:09.175602Z",
     "start_time": "2023-07-17T15:51:09.171496Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_sample = pd.concat([\n",
    "    pd_all[pd_all.label == 1].sample(50),\n",
    "    pd_all[pd_all.label == 0].sample(50)\n",
    "])\n",
    "pd_sample.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e143d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:09.181479Z",
     "start_time": "2023-07-17T15:51:09.176430Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 将文档中的每一个评论分为一行\n",
    "file_line = []\n",
    "\n",
    "count = 0  # 统计行数\n",
    "for line in range(0, pd_sample.shape[0]):\n",
    "    file_line.append(pd_sample.iloc[line]['review'])\n",
    "    count += 1\n",
    "\n",
    "print(\"There are %d rows in total.\" % count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6f6525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:09.184562Z",
     "start_time": "2023-07-17T15:51:09.182109Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f12329",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:09.501085Z",
     "start_time": "2023-07-17T15:51:09.185210Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 在过程中动态添加用户字典\n",
    "jieba.suggest_freq('碧海蓝天', True)\n",
    "# 也可以自己先形成一个文档例如mydict.txt\n",
    "# 用法： jieba.load_userdict(file_name) # file_name 为文件类对象或自定义词典的路径\n",
    "# 词典格式：一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。\n",
    "# file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。\n",
    "# 使用 add_word(word, freq=None, tag=None) 和 del_word(word) 可在程序中动态修改词典。\n",
    "\n",
    "# 使用jieba开始分词\n",
    "# file_userDict = 'dict.txt'  # 自定义的词典 目前还没有\n",
    "# jieba.load_userdict(file_userDict)\n",
    "res = []\n",
    "for i in range(len(file_line)):\n",
    "    sentence_seged = jieba.cut(file_line[i].strip())\n",
    "    res.append(' '.join(sentence_seged))\n",
    "\n",
    "print('Segmentation Complete!')\n",
    "\n",
    "for i in range(len(res)):\n",
    "    print(res[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eac3b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:09.504159Z",
     "start_time": "2023-07-17T15:51:09.501929Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载停用词列表\n",
    "f_stop = open('./stop_word.txt')  # 自己的中文停用词表\n",
    "sw = [line.strip() for line in f_stop]\n",
    "f_stop.close()\n",
    "# sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d613c4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:09.635147Z",
     "start_time": "2023-07-17T15:51:09.504764Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_list_seg = []\n",
    "\n",
    "for i in range(len(res)):\n",
    "    stopwords = sw\n",
    "    outstr = ''\n",
    "    for word in res[i].split():\n",
    "        # print('word:', word)\n",
    "        if word not in stopwords:\n",
    "            if word != '/t':\n",
    "                outstr += word\n",
    "                outstr += \" \"\n",
    "    print('Sentence %d, outstr: %s' % (i, outstr))\n",
    "    word_list_seg.append(outstr)\n",
    "\n",
    "print('_______________________')\n",
    "print('Stop Words Removal Complete!')\n",
    "print(len(word_list_seg))\n",
    "\n",
    "# for i in range(len(word_list_seg)):\n",
    "#    print(word_list_seg[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01757ee3",
   "metadata": {},
   "source": [
    "## SVD and LSI\n",
    "\n",
    "![](./svd.png)\n",
    "***Fig 2. SVD***\n",
    "\n",
    "***Latent Semantic Indexing (LSI)*** is a method for discovering hidden concepts in document data. Each document and term (word) is then expressed as a vector with elements corresponding to these concepts. Each element in a vector gives the degree of participation of the document or term in the corresponding concept. The goal is not to describe the concepts verbally, but to be able to represent the documents and terms in a unified way for exposing document-document, document-term, and term-term similarities or semantic relationship which are otherwise hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa6f4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:09.685296Z",
     "start_time": "2023-07-17T15:51:09.637511Z"
    }
   },
   "outputs": [],
   "source": [
    "# raw documents to tf-idf matrix:\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                             use_idf=True,\n",
    "                             smooth_idf=True)\n",
    "# SVD to reduce dimensionality:\n",
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=10)\n",
    "# pipeline of tf-idf + SVD, fit to and applied to documents:\n",
    "svd_transformer = Pipeline([('tfidf', vectorizer), ('svd', svd_model)])\n",
    "dc_matrix = svd_transformer.fit_transform(word_list_seg)\n",
    "# dc_matrix can later be used to compare documents, compare words, or compare queries with documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94572eca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:09.703001Z",
     "start_time": "2023-07-17T15:51:09.686806Z"
    }
   },
   "outputs": [],
   "source": [
    "dc_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674edf45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:09.727742Z",
     "start_time": "2023-07-17T15:51:09.706582Z"
    }
   },
   "outputs": [],
   "source": [
    "svd_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cbe21d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:09.738867Z",
     "start_time": "2023-07-17T15:51:09.729791Z"
    }
   },
   "outputs": [],
   "source": [
    "sum(svd_model.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce96328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:09.748703Z",
     "start_time": "2023-07-17T15:51:09.740456Z"
    }
   },
   "outputs": [],
   "source": [
    "document_concept_matrix = pd.DataFrame(dc_matrix)\n",
    "\n",
    "d = []\n",
    "for row in range(0, document_concept_matrix.shape[0]):\n",
    "    d.append(f'd{row+1}')\n",
    "document_concept_matrix.index = d\n",
    "\n",
    "tc_matrix = np.dot(svd_model.components_.T,\n",
    "                   np.diag(svd_model.singular_values_))\n",
    "term_concept_matrix = pd.DataFrame(tc_matrix)\n",
    "term_concept_matrix.index = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6952ddb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:09.766798Z",
     "start_time": "2023-07-17T15:51:09.750196Z"
    }
   },
   "outputs": [],
   "source": [
    "document_term = pd.concat([document_concept_matrix, term_concept_matrix])\n",
    "document_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e72f67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:09.996084Z",
     "start_time": "2023-07-17T15:51:09.769034Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot all document-concept vectors\n",
    "plt.scatter(x=document_concept_matrix[0], y=document_concept_matrix[1])\n",
    "# add labels to all points\n",
    "for idx, row in document_concept_matrix.iterrows():\n",
    "    plt.text(row[0], row[1], idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb098b37",
   "metadata": {},
   "source": [
    "## Text Similarity\n",
    "\n",
    "**Seaborn** is a library for making statistical graphics in Python. It builds on top of *matplotlib* and integrates closely with pandas data structures.\n",
    "\n",
    "**Seaborn** helps you explore and understand your data. Its plotting functions operate on *dataframes* and arrays containing whole datasets and internally perform the necessary semantic mapping and statistical aggregation to produce informative plots. Its dataset-oriented, declarative *API* lets you focus on what the different elements of your plots mean, rather than on the details of how to draw them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4392bd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:10.585883Z",
     "start_time": "2023-07-17T15:51:10.002880Z"
    }
   },
   "outputs": [],
   "source": [
    "def similar_matrix(truncated_text_vector, similarity_function):\n",
    "    le = len(truncated_text_vector)\n",
    "    matrix = [[\n",
    "        similarity_function(\n",
    "            [truncated_text_vector[i], truncated_text_vector[j]])[1, 0]\n",
    "        for j in range(le)\n",
    "    ] for i in range(le)]\n",
    "    sns.heatmap(matrix, center=1, annot=False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "similar_matrix(dc_matrix, cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b898cb",
   "metadata": {},
   "source": [
    "## Text Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8397c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:10.616900Z",
     "start_time": "2023-07-17T15:51:10.586600Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = KMeans(n_clusters=4)\n",
    "\n",
    "kmeans_results = clf.fit_predict(dc_matrix)\n",
    "kmeans_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e4e0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:51:10.771973Z",
     "start_time": "2023-07-17T15:51:10.617854Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(x=document_concept_matrix[0],\n",
    "            y=document_concept_matrix[1],\n",
    "            c=kmeans_results)\n",
    "for idx, row in document_concept_matrix.iterrows():\n",
    "    plt.text(row[0], row[1], idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee1a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
